{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd1abb40-e17f-44bf-9b4d-77fe55210b8c",
   "metadata": {},
   "source": [
    "### Annotation of Emotions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963b7fa8-9f37-4ca8-bc63-fb7ec07350aa",
   "metadata": {},
   "source": [
    "In this notebook, the annotation process of the dataset will be automated. To do this, we send the data to OpenAI with GPT40-mini using the Few Shot method and receive it annotated based on the emotions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652ef87d-a1ee-4196-bb08-d8790a073a9e",
   "metadata": {},
   "source": [
    "ref: https://social-media-lab.net/processing/classification.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5ab123b-b50c-4faf-8ff7-23fbf52fa1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q openai backoff gpt-cost-estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "935d92be-16d6-4141-a6a2-8d65821c9f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "import backoff\n",
    "import re\n",
    "import ast\n",
    "from gpt_cost_estimator import CostEstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b0444f1-e9b2-41c7-88e8-f32e53bb555d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6n/knlq_sss74g85w6tq1mnlfk40000gn/T/ipykernel_43782/1319260943.py:2: DtypeWarning: Columns (0,1,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_all = pd.read_csv(\"../Dataset/annotation_emotion_and_label_bert.csv\", delimiter=\",\")\n"
     ]
    }
   ],
   "source": [
    "df_test = pd.read_csv(\"../Dataset/label_annotation.csv\", delimiter=\";\")\n",
    "df_all = pd.read_csv(\"../Dataset/annotation_emotion_and_label_bert.csv\", delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17bb3ab9-1681-4186-8bf7-d9646b2b50d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For test purposes\n",
    "df = df_test\n",
    "# For all data\n",
    "#df = df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9312c337-76d4-4f46-b23e-d3642bdf086e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1041\n"
     ]
    }
   ],
   "source": [
    "print(len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4ced8b-8880-4088-992d-76795c6bb42d",
   "metadata": {},
   "source": [
    "## Setup GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "588cc4c3-d746-4eee-99cd-d80c40b10167",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_text = \"\"\"\n",
    "Example 1:\n",
    "Tweet: \"@newsmax @bennyjohnson Biden plans to tax your 401K. Believe it. The democrats are spending so much money they need new revenue sources. Wake up America. This is YOUR MONEY!!!!\"\n",
    "Valence: Negative\n",
    "Arousal: High\n",
    "Example 2:\n",
    "Tweet: \"It is profoundly wrong that the Walton family of Walmart owns more wealth than the bottom 130 million Americans.\"\n",
    "Valence: Negative\n",
    "Arousal: Low\n",
    "Example 3:\n",
    "Tweet: \"Congrats to Ivy Taylor, first African American mayor of San Antonio.\"\n",
    "Valence: Positive\n",
    "Arousal: Low\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ffc05d33-6619-499b-8b3b-ca9c6965e262",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are an advanced AI model for classification. Your task is to annotate tweets by identifying the emotional tone based on two dimensions: valence (positive, neutral, or negative) and arousal (low, medium, or high). The goal is to analyze how understanding emotions in tweets can help us to combat fake news. The coding helps to understand how emotions are mapped in tweets and what this means for the spread of fake news.\n",
    "\n",
    "The posts should be classified into 2 x 3 categories. Multiple answers are not possible in the individual blocks. The theoretical basis of my work assumes that users pursue the following emotions in social media:\n",
    "\n",
    "1. Valence (Emotional Tone):\n",
    "Positive: The tweet conveys positive emotions like happiness, excitement, or hope.\n",
    "Neutral: The tweet doesn’t express strong emotional content or is emotionally neutral.\n",
    "Negative: The tweet expresses negative emotions like anger, fear, or sadness.\n",
    "2. Arousal (Emotional Intensity):\n",
    "High: The tweet conveys strong, intense emotions. These tweets may feel urgent, excited, or highly emotional.\n",
    "Medium: The tweet shows moderate emotional intensity. The emotion is present but not overwhelming.\n",
    "Low: The tweet conveys mild or subdued emotions, such as calmness or slight sadness.\n",
    "\n",
    "General Approach to Borderline Cases:\n",
    "Focus on the most prominent emotion when deciding between two categories.\n",
    "If the tweet seems to fall between two levels of arousal (e.g., between medium and high), consider the context:\n",
    "Does the tweet show urgency or emotional intensity? (If yes, lean towards high arousal).\n",
    "Is the tone emotional but more measured? (If yes, lean towards medium arousal).\n",
    "\n",
    "Here are some classification Examples:\n",
    "{example_text}\n",
    "\n",
    "\n",
    "During classification, you receive the text of the tweet. Your task is to decide which level of valence (positive, neutral, negative) and arousal (high, medium, low) is present in the post. Return the applicable categories as a list.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f0be327-241e-41e5-8064-1a3a51636c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = system_prompt.replace('{example_text}',example_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba95620e-47e3-45c7-a21f-8bc4c6524df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Please categorise the following tweet in the social media for the Valence category as ‘negative’, ‘neutral’ or ‘positive’.\n",
    "For the Arousal category as, ‘high’, ‘low’, ‘medium’\n",
    "Your answer for each category MUST be one of [‘negative’, ‘neutral’, ‘positive’] for Valence [‘high’, ‘low’, ‘medium’] for arousal and should be in lower case.\n",
    "Text: [TEXT]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ac268def-33b2-4a6d-987a-00682af09d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key_name = \"NLP\"\n",
    "api_key = \"\" # INSERT KEY\n",
    "# Initialize OpenAI using the key\n",
    "client = OpenAI(\n",
    "    api_key=api_key\n",
    ")\n",
    "\n",
    "@CostEstimator()\n",
    "def query_openai(model, temperature, messages, mock=True, completion_tokens=10):\n",
    "    return client.chat.completions.create(\n",
    "                      model=model,\n",
    "                      temperature=temperature,\n",
    "                      messages=messages,\n",
    "                      max_tokens=600)\n",
    "\n",
    "# We define the run_request method to wrap it with the @backoff decorator\n",
    "@backoff.on_exception(backoff.expo, (openai.RateLimitError, openai.APIError))\n",
    "def run_request(system_prompt, user_prompt, model, mock):\n",
    "  messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": user_prompt}\n",
    "  ]\n",
    "\n",
    "  return query_openai(\n",
    "          model=model,\n",
    "          temperature=0.0,\n",
    "          messages=messages,\n",
    "          mock=mock\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847ce7bb-e82c-4588-b6ae-996aea0ed6bd",
   "metadata": {},
   "source": [
    "## Running the Request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fabe801d-e3f3-44d8-955d-11fcb0cf23c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reset Cost Estimation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4d047efc5dc4d43b59c5d9659809d33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1041 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost: $0.0007 | Total: $0.7185\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "#@markdown Do you want to mock the OpenAI request (dry run) to calculate the estimated price?\n",
    "MOCK = True # @param {type: \"boolean\"}\n",
    "#@markdown Do you want to reset the cost estimation when running the query?\n",
    "RESET_COST = True # @param {type: \"boolean\"}\n",
    "#@markdown What's the column name to save the results of the data extraction task to?\n",
    "COLUMN = 'Sentiment' # @param {type: \"string\"}\n",
    "#@markdown Do you want to run the request on a smaller sample of the whole data? (Useful for testing). Enter 0 to run on the whole dataset.\n",
    "SAMPLE_SIZE = len(df) # @param {type: \"number\", min: 0}\n",
    "id = 0\n",
    "#@markdown Which model do you want to use?\n",
    "MODEL = \"gpt-3.5-turbo-1106\"\t\n",
    "\n",
    "\n",
    "# Initializing the empty column\n",
    "if COLUMN not in df.columns:\n",
    "  df[COLUMN] = None\n",
    "\n",
    "# Reset Estimates\n",
    "CostEstimator.reset()\n",
    "print(\"Reset Cost Estimation\")\n",
    "\n",
    "filtered_df = df.copy()\n",
    "\n",
    "# Skip previously annotated rows\n",
    "filtered_df = filtered_df[pd.isna(filtered_df[COLUMN])]\n",
    "\n",
    "if SAMPLE_SIZE > 0:\n",
    "  filtered_df = filtered_df.sample(SAMPLE_SIZE)\n",
    "\n",
    "for index, row in tqdm(filtered_df.iterrows(), total=len(filtered_df)):\n",
    "    try:\n",
    "        \n",
    "        p = prompt.replace('[TEXT]', row['tweet'])\n",
    "        response = run_request(system_prompt, p, MODEL, MOCK)\n",
    "        if not MOCK:\n",
    "          # Extract the response content\n",
    "          # Adjust the following line according to the structure of the response\n",
    "          r = response.choices[0].message.content\n",
    "          # Update the 'new_df' DataFrame\n",
    "          df.at[index, COLUMN] = r\n",
    "          if id%1000 == 0:\n",
    "                df.to_csv(f\"annotations/{id}.csv\")\n",
    "          id +=1\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        # Optionally, handle the error (e.g., by logging or by setting a default value)\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12e5367-7857-4237-96ec-fd4eb85b49df",
   "metadata": {},
   "source": [
    "### Process Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f124dde3-7a3a-4241-8879-212fcdc3d086",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_one_valence = r\"(?<=valence: )[a-z]+\"\n",
    "reg_two_arousal = r\"(?<=arousal: )[a-z]+\"\n",
    "\n",
    "def checkt_contain(text, category):\n",
    "\n",
    "    try:\n",
    "        categories = {\n",
    "            'arousal':[\"low\",\"medium\",\"high\"],\n",
    "            'valence':[\"negative\", \"neutral\",\"positive\"]\n",
    "        }\n",
    "\n",
    "        for label in categories[category]:\n",
    "            if label in text:\n",
    "                return label\n",
    "    except:\n",
    "        print(text)\n",
    "    \n",
    "\n",
    "def format_str(text):\n",
    "    arousal = ''\n",
    "    valence = ''\n",
    "    text = str(text).lower()\n",
    "    if '[' in text:\n",
    "        try:\n",
    "            return ast.literal_eval(text)\n",
    "        except:\n",
    "            pattern = r'valence:\\s*(\\w+).*arousal:\\s*(\\w+)'\n",
    "            match = re.search(pattern, text)\n",
    "            if match:\n",
    "                valence, arousal = match.groups()\n",
    "                return [valence, arousal]\n",
    "            else:\n",
    "                standardized_string = text.replace('‘', \"'\").replace('’', \"'\")\n",
    "                return ast.literal_eval(standardized_string)\n",
    "\n",
    "    else:\n",
    "        match_valence = re.search(reg_one_valence, text)\n",
    "        match_arousal = re.search(reg_two_arousal, text)\n",
    "\n",
    "        if match_valence:\n",
    "            valence = match_valence.group(0)\n",
    "        else:\n",
    "            valence = checkt_contain(text, 'valence')\n",
    "        if match_arousal:\n",
    "            arousal = match_arousal.group(0)\n",
    "        else:\n",
    "            arousal = checkt_contain(text, 'arousal')\n",
    "\n",
    "    return [valence, arousal]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4c0e70b6-4f93-4f17-b3f3-c491097d0e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "valence_list = []\n",
    "arousal_list = []\n",
    "for data in df['Sentiment']:\n",
    "    valence, arousal = format_str(data)\n",
    "    valence_list.append(valence)\n",
    "    arousal_list.append(arousal)\n",
    "df['valence'] = valence_list\n",
    "df['arousal'] = arousal_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6f8c44ab-a0af-4dc0-b37b-f3bd422ae445",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['id','tweet', 'valence', 'arousal']].to_csv(\"../Dataset/GPT-3.5.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362a75fb-53eb-480b-9b12-90ed91c0fd4f",
   "metadata": {},
   "source": [
    "### GPT 4o mini"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbc7301-2f3a-4eda-8239-7e1facd46fd3",
   "metadata": {},
   "source": [
    "unfortunately the approach above does not work with GPT 4o-mini. Instead we are using this code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2f332d09-0512-4962-a6c8-406a9dd54165",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=api_key)\n",
    "def call_api(p):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": p}\n",
    "        ]\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "01a86c61-eba6-4458-abe8-3ec11ca8cffd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4b0d066a55d423583cfc9e042bc81fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1041 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "COLUMN = 'GPT4mini'\n",
    "df[COLUMN] = None\n",
    "for index, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        p = prompt.replace('[TEXT]', row['tweet'])\n",
    "        response = call_api(p)\n",
    "        df.at[index, COLUMN] = response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ca05e6-cabc-4ade-95cc-6190f6d01a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "valence_list = []\n",
    "arousal_list = []\n",
    "for data in df['GPT4mini']:\n",
    "    valence, arousal = format_str(data)\n",
    "    valence_list.append(valence)\n",
    "    arousal_list.append(arousal)\n",
    "df['valence'] = valence_list\n",
    "df['arousal'] = arousal_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8507e4f7-67c5-46d1-a662-71578f542c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../Dataset/GPT4omini.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b322d5df-6251-48bc-99ad-f8d0387e9c04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
